{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these flags to train a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RESNET = False\n",
    "TRAIN_UNODE = True\n",
    "TRAIN_UNET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "import scipy.ndimage\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from models import ConvODEUNet, ConvResUNet, ODEBlock, Unet\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Warwick QU Dataset (Released 2016_07_08)'):\n",
    "    !wget https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/warwick_qu_dataset_released_2016_07_08.zip\n",
    "    !unzip warwick_qu_dataset_released_2016_07_08.zip     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setNumThreads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentations import ElasticTransformations, RandomRotationWithMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLaSDataLoader(object):\n",
    "    def __init__(self, patch_size, dataset_repeat=1, images=np.arange(0, 70), validation=False):\n",
    "        self.image_fname = 'Warwick QU Dataset (Released 2016_07_08)/train_' \n",
    "        self.images = images\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.repeat = dataset_repeat\n",
    "        self.validation = validation\n",
    "        \n",
    "        self.image_mask_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomVerticalFlip(),\n",
    "            RandomRotationWithMask(45, resample=False, expand=False, center=None),\n",
    "            ElasticTransformations(2000, 60),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        self.image_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # index to image index\n",
    "        index_img = index // self.repeat\n",
    "        index_img = self.images[index_img]\n",
    "        index_str = str(index_img.item() + 1)\n",
    "        \n",
    "        image = self.image_fname + index_str + '.bmp'\n",
    "        mask = self.image_fname + index_str + '_anno.bmp'\n",
    "        \n",
    "        image = PIL.Image.open(image)\n",
    "        ratio = (775 / 512)\n",
    "        new_size = (int(round(image.size[0] / ratio)), \n",
    "                    int(round(image.size[1] / ratio)))\n",
    "        \n",
    "        image = image.resize(new_size)\n",
    "        \n",
    "        mask = PIL.Image.open(mask)\n",
    "        mask = mask.resize(new_size)\n",
    "        \n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        if not self.validation:\n",
    "            pad_h = max(self.patch_size[0] - image.shape[0], 128) \n",
    "            pad_w = max(self.patch_size[1] - image.shape[1], 128) # pad to image size\n",
    "        else: \n",
    "            pad_h = max((self.patch_size[0] - image.shape[0]) // 2 + 1, 0)\n",
    "            pad_w = max((self.patch_size[1] - image.shape[1]) // 2 + 1, 0)\n",
    "            \n",
    "        # pad to image size\n",
    "        padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='reflect')\n",
    "        mask = np.pad(mask, ((pad_h, pad_h), (pad_w, pad_w)), mode='reflect')\n",
    "\n",
    "        if not self.validation:\n",
    "            loc_y = random.randint(0, padded_image.shape[0] - self.patch_size[0])        \n",
    "            loc_x = random.randint(0, padded_image.shape[1] - self.patch_size[1])  \n",
    "        else:\n",
    "            loc_y, loc_x = 0, 0\n",
    "            \n",
    "        patch = torch.from_numpy(padded_image.transpose(2, 0, 1)).float() / 255\n",
    "        n_glands = mask.max()\n",
    "        label = torch.from_numpy(mask).float() / n_glands\n",
    "\n",
    "        if not self.validation:            \n",
    "            patch_label_concat = torch.cat((patch, label[None, :, :].float()))\n",
    "            patch_label_concat = self.image_mask_transforms(patch_label_concat)\n",
    "            patch, label = patch_label_concat[0:3], np.round(patch_label_concat[3] * n_glands)\n",
    "            patch = self.image_transforms(patch)\n",
    "        else:\n",
    "            label *= n_glands\n",
    "            \n",
    "        boundaries = torch.zeros(label.shape)\n",
    "        for i in np.unique(mask):\n",
    "            if i == 0: continue\n",
    "            gland_mask = (label == i).float()\n",
    "            binarized_mask_border = scipy.ndimage.morphology.binary_erosion(gland_mask, \n",
    "                                                                            structure=np.ones((13, 13)), \n",
    "                                                                            border_value=1)\n",
    "            \n",
    "            binarized_mask_border = torch.from_numpy(binarized_mask_border.astype(np.float32))\n",
    "            boundaries[label == i] = binarized_mask_border[label == i]\n",
    "        \n",
    "        label = (label > 0).float()\n",
    "        label = torch.stack((boundaries, label))\n",
    "            \n",
    "        patch = patch[:, loc_y:loc_y+self.patch_size[0], loc_x:loc_x+self.patch_size[1]]   \n",
    "        label = label[:, loc_y:loc_y+self.patch_size[0], loc_x:loc_x+self.patch_size[1]]\n",
    "        \n",
    "        return patch, label.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) * self.repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "val_set_idx = torch.LongTensor(10).random_(0, 85)\n",
    "train_set_idx = torch.arange(0, 85)\n",
    "\n",
    "overlapping = (train_set_idx[..., None] == val_set_idx).any(-1)\n",
    "train_set_idx = torch.masked_select(train_set_idx, 1-overlapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows users: you may need to put the dataloader inside a different python file when using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = GLaSDataLoader((352, 512), dataset_repeat=1, images=train_set_idx)\n",
    "valset = GLaSDataLoader((352, 512), dataset_repeat=1, images=val_set_idx, validation=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=10)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n",
    "\n",
    "for y in range(5):\n",
    "    for x in range(3):\n",
    "        sample = trainset[y]\n",
    "        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n",
    "        ax[y, x * 2 + 1].imshow(sample[1][0])\n",
    "        ax[y, x * 2].axis('off')\n",
    "        ax[y, x * 2 + 1].axis('off')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24, 15))\n",
    "\n",
    "sample = trainset[0]\n",
    "ax[1].imshow(sample[1][0].numpy())\n",
    "ax[2].imshow(sample[1].sum(dim=0))\n",
    "ax[0].imshow(sample[0].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n",
    "\n",
    "for y in range(5):\n",
    "    for x in range(3):\n",
    "        sample = valset[y]\n",
    "        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n",
    "        ax[y, x * 2 + 1].imshow(sample[1][1])\n",
    "        ax[y, x * 2].axis('off')\n",
    "        ax[y, x * 2 + 1].axis('off')\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "if TRAIN_UNODE:\n",
    "    net = ConvODEUNet(num_filters=16, output_dim=2, time_dependent=True, \n",
    "                      non_linearity='lrelu', adjoint=True, tol=1e-3)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_RESNET:\n",
    "    net = ConvResUNet(num_filters=16, output_dim=2, non_linearity='lrelu')\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_UNET:\n",
    "    net = Unet(depth=5, in_ch=3, out_ch=64, n_classes=2).cuda()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in net.modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "val_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "if TRAIN_UNET:\n",
    "    cross_entropy = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def criterion(conf, labels):\n",
    "        out_shape = conf.shape[2:4]\n",
    "        label_shape = labels.shape[2:4]\n",
    "\n",
    "        w = (label_shape[1] - out_shape[1]) // 2  # net.crop_left_top\n",
    "        h = (label_shape[1] - out_shape[1]) // 2\n",
    "        dh, dw = out_shape[0:2]\n",
    "\n",
    "        conf_loss_ce = cross_entropy(conf, labels[:, :, h:h+dh, w:w+dw])\n",
    "\n",
    "        return conf_loss_ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "nfe = [[],[],[],[],[],[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulate_batch = 8  # mini-batch size by gradient accumulation\n",
    "accumulated = 0\n",
    "\n",
    "if TRAIN_RESNET: filename = 'best_border_resnet_model.pt'\n",
    "elif TRAIN_UNODE: filename = 'best_border_unode_model.pt'\n",
    "elif TRAIN_UNET: filename = 'best_border_unet_model.pt'\n",
    "\n",
    "def run(lr=1e-3, epochs=100):\n",
    "    accumulated = 0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training loop with gradient accumulation\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for data in tqdm(trainloader):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels) / accumulate_batch\n",
    "            loss.backward()\n",
    "            accumulated += 1\n",
    "            if accumulated == accumulate_batch:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                accumulated = 0\n",
    "\n",
    "            running_loss += loss.item() * accumulate_batch\n",
    "\n",
    "        losses.append(running_loss / len(trainloader))\n",
    "        \n",
    "        # validation loop\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for data in valloader:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            val_losses.append(running_loss / len(valloader))\n",
    "            if np.argmin(val_losses) == len(val_losses) - 1 and loss < 0.4:\n",
    "                torch.save(net, filename)\n",
    "                \n",
    "            plot_losses(inputs, outputs)\n",
    "                \n",
    "def plot_losses(inputs, outputs):\n",
    "    # plot statistics\n",
    "    if TRAIN_UNODE:\n",
    "        nfe[0].append(net.odeblock_down1.odefunc.nfe)\n",
    "        nfe[1].append(net.odeblock_down2.odefunc.nfe)\n",
    "        nfe[2].append(net.odeblock_down3.odefunc.nfe)\n",
    "        nfe[3].append(net.odeblock_down4.odefunc.nfe)\n",
    "        nfe[4].append(net.odeblock_embedding.odefunc.nfe)\n",
    "        nfe[5].append(net.odeblock_up1.odefunc.nfe)\n",
    "        nfe[6].append(net.odeblock_up2.odefunc.nfe)\n",
    "        nfe[7].append(net.odeblock_up3.odefunc.nfe)\n",
    "        nfe[8].append(net.odeblock_up4.odefunc.nfe)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if TRAIN_UNODE: cols = 4\n",
    "    else: cols = 3\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15,5))\n",
    "\n",
    "    if TRAIN_UNODE: fig.suptitle('U-NODE', fontsize=16)\n",
    "    elif TRAIN_RESNET: fig.suptitle('RESNET', fontsize=16)\n",
    "    elif TRAIN_UNET: fig.suptitle('UNET', fontsize=16)\n",
    "\n",
    "    ax[0].plot(np.arange(len(losses)), losses, label=\"loss\")\n",
    "    ax[0].plot(np.arange(len(val_losses)), val_losses, label=\"val_loss\")\n",
    "\n",
    "    if TRAIN_UNODE:\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[0], label=\"down1\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[1], label=\"down2\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[2], label=\"down3\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[3], label=\"down4\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[4], label=\"embed\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[5], label=\"up1\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[6], label=\"up2\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[7], label=\"up3\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[8], label=\"up4\")\n",
    "        ax[3].legend() \n",
    "\n",
    "\n",
    "    outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1)[0]\n",
    "    outputs = outputs.detach().cpu()\n",
    "    outputs = outputs.numpy()\n",
    "\n",
    "    ax[0].legend() \n",
    "    ax[1].imshow(outputs)\n",
    "    ax[2].imshow(inputs.detach().cpu()[0].numpy().transpose(1,2,0))\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_UNODE or TRAIN_RESNET: lr = 1e-3 \n",
    "else: lr = 1e-4\n",
    "\n",
    "run(lr, 600 - len(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "net = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(valloader):\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"Check validation loss:\", running_loss / len(valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import inference_image, postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(4*3,3*5))\n",
    "\n",
    "ax[0, 0].set_title('Image')\n",
    "ax[0, 1].set_title('Ground-truth')\n",
    "ax[0, 2].set_title('Trained network')\n",
    "\n",
    "for col in range(3):\n",
    "    for row in range(5):\n",
    "        index = val_set_idx[row]\n",
    "        image = PIL.Image.open(f'Warwick QU Dataset (Released 2016_07_08)/train_{index}.bmp')\n",
    "        gt = PIL.Image.open(f'Warwick QU Dataset (Released 2016_07_08)/train_{index}_anno.bmp')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result, input_image = inference_image(net, image, shouldpad=TRAIN_UNET)\n",
    "            result = postprocess(result, gt)\n",
    "        if col == 0:\n",
    "            ax[row, col].imshow(image)\n",
    "        elif col == 1:\n",
    "            ax[row, col].imshow(np.array(gt) > 0)\n",
    "        else:\n",
    "            ax[row, col].imshow(image)\n",
    "            ax[row, col].imshow(result, alpha=0.5)\n",
    "                \n",
    "        ax[row, col].set_axis_off()\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import ObjectDice, ObjectHausdorff, F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RESNET = False\n",
    "TEST_UNODE = True\n",
    "TEST_UNET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_UNODE: net = torch.load('best_border_unode_model.pt')\n",
    "elif TEST_RESNET: net = torch.load('best_border_resnet_model.pt')\n",
    "elif TEST_UNET: net = torch.load('best_border_unet_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dice, hausdorff, f1, dice_full = 0, 0, 0, 0\n",
    "\n",
    "if TEST_UNODE: folder = 'results_unode'\n",
    "elif TEST_UNET: folder = 'results_unet'\n",
    "elif TEST_RESNET: folder = 'results_resnet'\n",
    "    \n",
    "images = []\n",
    "for index in np.arange(1, 81):\n",
    "    if index < 61: images.append(f'testA_{index}_anno.bmp')\n",
    "    else: images.append(f'testB_{index - 60}_anno.bmp')\n",
    "        \n",
    "for i, fname in tqdm_notebook(enumerate(images), total=80):\n",
    "    gt = PIL.Image.open(f'Warwick QU Dataset (Released 2016_07_08)/' + fname)\n",
    "    image = PIL.Image.open(f'Warwick QU Dataset (Released 2016_07_08)/' + fname.replace('_anno', ''))\n",
    "    result, resized = inference_image(net, image, shouldpad=TEST_UNET)\n",
    "    result = postprocess(result, gt)\n",
    "\n",
    "    gt = skimage.measure.label(np.array(gt))\n",
    "    \n",
    "    f1_img = F1score(result, gt)\n",
    "    hausdorff_img = ObjectHausdorff(result, gt)\n",
    "    dice_img = ObjectDice(result, gt)\n",
    "    \n",
    "    f1 += f1_img\n",
    "    hausdorff += hausdorff_img\n",
    "    dice += dice_img\n",
    "    \n",
    "    result = np.array(result) > 0\n",
    "    gt = np.array(gt) > 0\n",
    "    intersection = np.logical_and(result, gt)\n",
    "    \n",
    "    if i == 59:        \n",
    "        diceA = dice \n",
    "        hausdorffA = hausdorff \n",
    "        f1A = f1\n",
    "\n",
    "    \n",
    "    print(i, f1_img, hausdorff_img, dice_img)\n",
    "\n",
    "diceB = dice - diceA\n",
    "hausdorffB = hausdorff - hausdorffA\n",
    "f1B = f1 - f1A\n",
    "\n",
    "print('ObjectDice:', dice / 80, 'A', diceA / 60, 'B', diceB / 20)\n",
    "print('Hausdorff:', hausdorff / 80, 'A', hausdorffA / 60, 'B', hausdorffB / 20)\n",
    "print('F1:', f1 / 80, 'A', f1A / 60, 'B', f1B / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
