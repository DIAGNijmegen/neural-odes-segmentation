{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these flags to train a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_RESNET = False\n",
    "TRAIN_UNODE = True\n",
    "TRAIN_UNET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from models import ConvODEUNet, ConvResUNet, ODEBlock, Unet\n",
    "\n",
    "import torchvision\n",
    "import scipy.ndimage\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy.misc import imresize\n",
    "from IPython.display import clear_output\n",
    "from skimage.measure import label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/download/warwick_qu_dataset_released_2016_07_08.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip warwick_qu_dataset_released_2016_07_08.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.setNumThreads(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from augmentations import ElasticTransformations, RandomRotationWithMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GLaSDataLoader(object):\n",
    "    def __init__(self, patch_size, dataset_repeat=1, images=np.arange(0, 70), validation=False):\n",
    "        self.image_fname = 'Warwick QU Dataset (Released 2016_07_08)/train_' \n",
    "        self.images = images\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.repeat = dataset_repeat\n",
    "        self.validation = validation\n",
    "        \n",
    "        self.image_mask_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomVerticalFlip(),\n",
    "            RandomRotationWithMask(20, resample=False, expand=False, center=None),\n",
    "            ElasticTransformations(2000, 60),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "        self.image_transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "            torchvision.transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # generate filename using index\n",
    "        index_img = index // self.repeat\n",
    "        index_img = self.images[index_img]\n",
    "        index_str = str(index_img + 1)\n",
    "        \n",
    "        image = self.image_fname + index_str + '.bmp'\n",
    "        mask = self.image_fname + index_str + '_anno.bmp'\n",
    "        \n",
    "        # open and resize image and mask\n",
    "        image = Image.open(image)\n",
    "        ratio = (775 / 512)\n",
    "        new_size = (int(round(image.size[0] / ratio)), \n",
    "                    int(round(image.size[1] / ratio)))\n",
    "        \n",
    "        image = image.resize(new_size)\n",
    "        \n",
    "        mask = Image.open(mask)\n",
    "        mask = mask.resize(new_size)\n",
    "        \n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        # pad images if needed\n",
    "        if not self.validation:\n",
    "            pad_h = max(self.patch_size[0] - image.shape[0], 128) \n",
    "            pad_w = max(self.patch_size[1] - image.shape[1], 128)\n",
    "        else: \n",
    "            # we pad more during training for data augmentation by translation \n",
    "            pad_h = max((self.patch_size[0] - image.shape[0]) // 2 + 1, 0)\n",
    "            pad_w = max((self.patch_size[1] - image.shape[1]) // 2 + 1, 0)\n",
    "            \n",
    "        # pad to image size\n",
    "        padded_image = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='reflect')\n",
    "        mask = np.pad(mask, ((pad_h, pad_h), (pad_w, pad_w)), mode='reflect')\n",
    "\n",
    "        # randomly pick location when training\n",
    "        if not self.validation:\n",
    "            loc_y = random.randint(0, padded_image.shape[0] - self.patch_size[0])        \n",
    "            loc_x = random.randint(0, padded_image.shape[1] - self.patch_size[1])  \n",
    "        else:\n",
    "            loc_y = 0\n",
    "            loc_x = 0\n",
    "            \n",
    "        new_mask = np.zeros(mask.shape)\n",
    "\n",
    "        # create borders\n",
    "        for i in np.unique(mask):\n",
    "            if i == 0: continue\n",
    "            gland_mask = (mask == i).astype(np.float32)\n",
    "            binarized_mask_border = gland_mask - ndimage.morphology.binary_erosion(gland_mask, iterations=5, border_value=1)\n",
    "            gland_mask = gland_mask * 2 - binarized_mask_border\n",
    "            new_mask[mask == i] = gland_mask[mask == i]\n",
    "            \n",
    "        mask = new_mask\n",
    "        patch = torch.from_numpy(padded_image.transpose(2, 0, 1)).float() / 255\n",
    "        label = torch.from_numpy(mask) / 2\n",
    "                \n",
    "        # perform data augmentation on image and mask\n",
    "        if not self.validation:            \n",
    "            patch_label_concat = torch.cat((patch, label[None, :, :].float()))\n",
    "            patch_label_concat = self.image_mask_transforms(patch_label_concat)\n",
    "            patch, label = patch_label_concat[0:3], np.round(patch_label_concat[3] * 2)\n",
    "            patch = self.image_transforms(patch)\n",
    "        else:\n",
    "            label *= 2\n",
    "            \n",
    "        # crop to patch size\n",
    "        patch = patch[:, \n",
    "                      loc_y:loc_y+self.patch_size[0],\n",
    "                      loc_x:loc_x+self.patch_size[1]]   \n",
    "        label = label[loc_y:loc_y+self.patch_size[0],\n",
    "                      loc_x:loc_x+self.patch_size[1]]\n",
    "        \n",
    "        \n",
    "        return patch, label.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) * self.repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows users: you may need to put the dataloader inside a different python file if using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = GLaSDataLoader((352, 512), dataset_repeat=1)\n",
    "valset = GLaSDataLoader((352, 512), dataset_repeat=1, images=np.arange(80, 85), validation=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=6)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n",
    "\n",
    "for y in range(5):\n",
    "    for x in range(3):\n",
    "        sample = trainset[y]\n",
    "        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n",
    "        ax[y, x * 2 + 1].imshow(sample[1])\n",
    "        ax[y, x * 2].axis('off')\n",
    "        ax[y, x * 2 + 1].axis('off')\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n",
    "\n",
    "for y in range(5):\n",
    "    for x in range(3):\n",
    "        sample = valset[y]\n",
    "        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n",
    "        ax[y, x * 2 + 1].imshow(sample[1])\n",
    "        ax[y, x * 2].axis('off')\n",
    "        ax[y, x * 2 + 1].axis('off')\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "if TRAIN_UNODE:\n",
    "    hidden_dim = 16\n",
    "    net = ConvODEUNet(device, (3, 512, 512), hidden_dim, time_dependent=True, \n",
    "                      non_linearity='lrelu', augment_dim=hidden_dim - 3, adjoint=True, tol=1e-3)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_RESNET:\n",
    "    net = ConvResUNet(device, (3, 512, 512), 16, non_linearity='lrelu', output_dim=3)\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TRAIN_UNET:\n",
    "    net = Unet(5, 3, 64, 3)\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for m in net.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if TRAIN_UNET:\n",
    "    cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "    def criterion(conf, labels):\n",
    "        out_shape = conf.shape[2:4]\n",
    "        label_shape = labels.shape[1:3]\n",
    "\n",
    "        w = (label_shape[1] - out_shape[1]) // 2 \n",
    "        h = (label_shape[1] - out_shape[1]) // 2\n",
    "        dh, dw = out_shape[0:2]\n",
    "        conf_loss = cross_entropy(conf, labels[:, h:h+dh, w:w+dw].long())\n",
    "\n",
    "        return conf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "nfe = [[],[],[],[],[],[],[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accumulate_batch = 8  # mini-batch size by gradient accumulation\n",
    "accumulated = 0\n",
    "\n",
    "if TRAIN_RESNET: filename = 'best_border_resnet_model.pt'\n",
    "elif TRAIN_UNODE: filename = 'best_border_unode_model.pt'\n",
    "elif TRAIN_UNET: filename = 'best_border_unet_model.pt'\n",
    "\n",
    "def run(lr=1e-3, epochs=100):\n",
    "    accumulated = 0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training loop with gradient accumulation\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for data in tqdm(trainloader):\n",
    "            inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels) / accumulate_batch\n",
    "            loss.backward()\n",
    "            accumulated += 1\n",
    "            if accumulated == accumulate_batch:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                accumulated = 0\n",
    "\n",
    "            running_loss += loss.item() * accumulate_batch\n",
    "\n",
    "        losses.append(running_loss / len(trainloader))\n",
    "        \n",
    "        # validation loop\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            for data in valloader:\n",
    "                inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            val_losses.append(running_loss / len(valloader))\n",
    "            if np.argmin(val_losses) == len(val_losses) - 1 and loss < 0.4:\n",
    "                torch.save(net, filename)\n",
    "                \n",
    "            plot_losses()\n",
    "                \n",
    "def plot_losses():\n",
    "    # plot statistics\n",
    "    if TRAIN_UNODE:\n",
    "        nfe[0].append(net.odeblock_down1.odefunc.nfe)\n",
    "        nfe[1].append(net.odeblock_down2.odefunc.nfe)\n",
    "        nfe[2].append(net.odeblock_down3.odefunc.nfe)\n",
    "        nfe[3].append(net.odeblock_down4.odefunc.nfe)\n",
    "        nfe[4].append(net.odeblock_embedding.odefunc.nfe)\n",
    "        nfe[5].append(net.odeblock_up1.odefunc.nfe)\n",
    "        nfe[6].append(net.odeblock_up2.odefunc.nfe)\n",
    "        nfe[7].append(net.odeblock_up3.odefunc.nfe)\n",
    "        nfe[8].append(net.odeblock_up4.odefunc.nfe)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if TRAIN_UNODE: cols = 4\n",
    "    else: cols = 3\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15,5))\n",
    "\n",
    "    if TRAIN_UNODE: fig.suptitle('U-NODE', fontsize=16)\n",
    "    elif TRAIN_RESNET: fig.suptitle('RESNET', fontsize=16)\n",
    "    elif TRAIN_UNET: fig.suptitle('UNET', fontsize=16)\n",
    "\n",
    "    ax[0].plot(np.arange(len(losses)), losses, label=\"loss\")\n",
    "    ax[0].plot(np.arange(len(val_losses)), val_losses, label=\"val_loss\")\n",
    "\n",
    "    if TRAIN_UNODE:\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[0], label=\"down1\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[1], label=\"down2\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[2], label=\"down3\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[3], label=\"down4\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[4], label=\"embed\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[5], label=\"up1\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[6], label=\"up2\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[7], label=\"up3\")\n",
    "        ax[3].plot(np.arange(len(nfe[0])), nfe[8], label=\"up4\")\n",
    "        ax[3].legend() \n",
    "\n",
    "\n",
    "    outputs = torch.argmax(torch.softmax(outputs, dim=1), dim=1)[0]\n",
    "    outputs = outputs.detach().cpu()\n",
    "    outputs = outputs.numpy()\n",
    "\n",
    "    ax[0].legend() \n",
    "    ax[1].imshow(outputs)\n",
    "    ax[2].imshow(inputs.detach().cpu()[0].numpy().transpose(1,2,0))\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_UNODE or TRAIN_RESNET: lr = 1e-4 \n",
    "else: lr = 1e-5\n",
    "\n",
    "run(lr, 1200 - len(losses)) # 300 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load best model\n",
    "net = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(valloader):\n",
    "        inputs, labels = data[0].cuda(), data[1].cuda()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"Check validation loss:\", running_loss / len(valloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inference_utils import resize_image, pad_image, prepare_image, crop_result, inference_image, eval_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(4*3,3*5))\n",
    "\n",
    "ax[0, 0].set_title('Image')\n",
    "ax[0, 1].set_title('Ground-truth')\n",
    "ax[0, 2].set_title('Trained network')\n",
    "\n",
    "for col in range(3):\n",
    "    for row in range(5):\n",
    "        image = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/train_{row+81}.bmp')\n",
    "        ann = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/train_{row+81}_anno.bmp')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            segmentation, input_image = inference_image(image)\n",
    "        if col == 0:\n",
    "            ax[row, col].imshow(input_image)\n",
    "        elif col == 1:\n",
    "            ax[row, col].imshow(np.array(ann) > 0)\n",
    "        else:\n",
    "            ax[row, col].imshow(input_image)\n",
    "            ax[row, col].imshow(segmentation, alpha=0.5)\n",
    "                \n",
    "        ax[row, col].set_axis_off()\n",
    "\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metrics import ObjectDice, ObjectHausdorff, F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_RESNET = False\n",
    "TEST_UNODE = True\n",
    "TEST_UNET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if TEST_UNODE: net = torch.load('best_border_unode_model.pt')\n",
    "elif TEST_RESNET: net = torch.load('best_border_resnet_model.pt')\n",
    "elif TEST_UNET: net = torch.load('best_border_unet_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testA_images = np.arange(60) + 1\n",
    "testB_images = np.arange(20) + 1\n",
    "\n",
    "if TEST_UNODE: folder = 'results_unode'\n",
    "elif TEST_UNET: folder = 'results_unet'\n",
    "elif TEST_RESNET: folder = 'results_resnet'\n",
    "    \n",
    "os.makedirs(folder)\n",
    "\n",
    "for index in tqdm(testA_images):\n",
    "    image = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/testA_{index}.bmp')\n",
    "    result, resized = inference_image(image, TEST_UNET)\n",
    "    pil_img = Image.fromarray(result.astype(np.uint8) * 255)\n",
    "    pil_img.save(f'./{folder}/{index}.png')\n",
    "    \n",
    "for index in tqdm(testB_images):\n",
    "    image = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/testB_{index}.bmp')\n",
    "    result, resized = inference_image(image, TEST_UNET)\n",
    "    pil_img = Image.fromarray(result.astype(np.uint8) * 255)\n",
    "    pil_img.save(f'./{folder}/{index+60}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dice = 0\n",
    "hausdorff = 0\n",
    "f1 = 0\n",
    "\n",
    "testA_images = np.arange(60) + 1\n",
    "testB_images = np.arange(20) + 1\n",
    "\n",
    "if TRAIN_UNODE: folder = 'results_unode'\n",
    "elif TRAIN_UNET: folder = 'results_unet'\n",
    "elif TRAIN_RESNET: folder = 'results_resnet'\n",
    "\n",
    "for i in tqdm_notebook(testA_images):\n",
    "    gt = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/testA_{i}_anno.bmp')\n",
    "    result = Image.open(f'{folder}/{i}.png')\n",
    "\n",
    "    result = result.resize(gt.size)\n",
    "    labeled_result = label(np.array(result))\n",
    "    \n",
    "    Image.fromarray(labeled_result.astype(np.uint8)).save(f'./{folder}/{i}_labeled.png')\n",
    "\n",
    "    f1 += F1score(labeled_result, gt)\n",
    "    hausdorff += ObjectHausdorff(labeled_result, gt)\n",
    "    dice += ObjectDice(labeled_result, gt)\n",
    "\n",
    "diceA = dice \n",
    "hausdorffA = hausdorff \n",
    "f1A = f1\n",
    "\n",
    "for i in tqdm_notebook(testB_images):\n",
    "    gt = Image.open(f'augmented-neural-odes-master/Warwick QU Dataset (Released 2016_07_08)/testB_{i}_anno.bmp')\n",
    "    result = Image.open(f'{folder}/{i + 60}.png')\n",
    "\n",
    "    result = result.resize(gt.size)\n",
    "    labeled_result = label(np.array(result))\n",
    "    \n",
    "    Image.fromarray(labeled_result.astype(np.uint8)).save(f'./{folder}/{i+60}_labeled.png')\n",
    "    \n",
    "    f1 += F1score(labeled_result, gt)\n",
    "    hausdorff += ObjectHausdorff(labeled_result, gt)\n",
    "    dice += ObjectDice(labeled_result, gt)\n",
    "\n",
    "diceB = dice - diceA\n",
    "hausdorffB = hausdorff - hausdorffA\n",
    "f1B = f1 - f1A\n",
    "\n",
    "print('ObjectDice:', dice / 80, 'A', diceA / 60, 'B', diceB / 20)\n",
    "print('Hausdorff:', hausdorff / 80, 'A', hausdorffA / 60, 'B', hausdorffB / 20)\n",
    "print('F1:', f1 / 80, 'A', f1A / 60, 'B', f1B / 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
